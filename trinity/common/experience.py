# -*- coding: utf-8 -*-
"""Experience Class."""
from __future__ import annotations

import pickle
import uuid
from dataclasses import dataclass, field
from typing import List, Optional

import torch
from torch import Tensor


@dataclass
class EID:
    """Experience ID class to uniquely identify an experience."""

    # TODO: do we need to add project/name here to make it unique across different projects?
    batch: int = 0  # Batch number, e.g., the explorer step num
    task: int = 0  # Task sequence in the batch, e.g., the first task in the batch has task=0
    run: int = 0  # Run id, e.g., the first run in the task has run=0
    step: int = 0  # Step number when running the task, e.g., the first step in the task has step=0
    suffix: str = field(
        default_factory=lambda: uuid.uuid4().hex[:6]
    )  # Unique identifier suffix, e.g., a UUID

    @property
    def uid(self) -> str:
        """An unique identifier for the experience."""
        return f"{self.batch}/{self.task}/{self.run}/{self.step}/{self.suffix}"

    @property
    def sid(self) -> str:
        """Step ID of the experience.

        For example, experiences generated by all runs of a same task at the same step will have the same sid.
        """
        return f"{self.batch}/{self.task}/{self.step}"

    @property
    def rid(self) -> str:
        """Run ID of the experience.

        For example, experiences generated by one run of a task at all steps will have the same run_id.
        """
        return f"{self.batch}/{self.task}/{self.run}"

    @property
    def gid(self) -> str:
        """Group ID for the experience.

        For example, experiences generated by a group run in GRPO-like algorithms will have the same gid.
        """
        return f"{self.batch}/{self.task}"

    def __str__(self):
        return self.uid

    def __repr__(self):
        return f"EID(batch={self.batch}, task={self.task}, run={self.run}, step={self.step}, uuid={self.suffix})"


@dataclass
class Experience:
    token_ids: Tensor  # [seq_length]
    eid: EID = field(default_factory=EID)  # Unique identifier for the experience
    reward: Optional[float] = None
    info: Optional[dict] = None
    metrics: Optional[dict[str, float]] = None

    def serialize(self) -> bytes:
        """Serialize the experience to bytes."""
        return pickle.dumps(self)

    @classmethod
    def deserialize(cls, data: bytes) -> Experience:
        return pickle.loads(data)


@dataclass
class SingleTurnExperience(Experience):
    """A single-turn prompt-response experience."""

    # Length of the prompt in tokens, used for generating attention masks
    prompt_length: int  # type: ignore[misc]
    response_text: Optional[str] = None  # Text of the response
    prompt_text: Optional[str] = None  # Text of the prompt
    logprobs: Optional[Tensor] = None  # [seq]

    @property
    def action_mask(self) -> Tensor:
        """Get the action mask for the single-turn experience."""
        # set the prompt length to 0 and the rest to 1
        action_mask = torch.zeros_like(self.token_ids, dtype=torch.bool)
        action_mask[self.prompt_length :] = 1
        return action_mask

    @classmethod
    def gather(cls, experiences: List[SingleTurnExperience], pad_token_id: int = 0) -> Experiences:
        if len(experiences) == 0:
            return empty_experiences()
        max_prompt_length = max([exp.prompt_length for exp in experiences])  # type: ignore [type-var]
        max_response_length = max([len(exp.tokens) - exp.prompt_length for exp in experiences])  # type: ignore [operator]
        eids = [exp.eid for exp in experiences]

        # Gather token_ids
        token_ids = gather_token_ids(
            experiences, max_prompt_length, max_response_length, pad_token_id
        )

        # Gather rewards
        if experiences[0].reward is not None:
            rewards = torch.tensor([exp.reward for exp in experiences], dtype=torch.float)
        else:
            rewards = None

        # gather action_masks
        action_masks = gather_action_masks(experiences, max_prompt_length, max_response_length)

        # gather attention_masks
        attention_masks = gather_attention_masks(
            experiences, max_prompt_length, max_response_length
        )

        # gather logprobs

        if all(exp.logprobs is not None for exp in experiences):
            logprobs = gather_logprobs(experiences, max_prompt_length, max_response_length)
        else:
            logprobs = None

        return Experiences(
            eids=eids,
            token_ids=token_ids,
            rewards=rewards,
            attention_masks=attention_masks,
            action_masks=action_masks,
            prompt_length=max_prompt_length,
            logprobs=logprobs,
        )


@dataclass
class MultiTurnExperience(Experience):
    """A multi-turn experience, which includes the conversation history in a single tensor."""

    # Action mask which indicates which tokens are generated by the model
    action_mask: Tensor  # type: ignore[misc]
    logprobs: Optional[Tensor] = None  # [seq_length]
    messages: Optional[List[dict]] = None  # List of messages in the conversation

    @property
    def prompt_length(self) -> int:
        return 1  # use action mask to determine the response tokens, set prompt_length to 1 to avoid no prompt issues

    @classmethod
    def gather(cls, experiences: List[SingleTurnExperience], pad_token_id: int = 0) -> Experiences:
        return SingleTurnExperience.gather(experiences, pad_token_id=pad_token_id)


@dataclass
class DPOExperience(Experience):
    """A DPO experience, which includes the chosen and rejected responses.

    `token_ids` should only contain the prompt tokens, while `chosen` and `rejected` should
    contain the response tokens.
    """

    # Token ids of the chosen response [resp_length]
    chosen: Tensor  # type: ignore[misc]
    # Token ids of the rejected response  [resp_length]
    rejected: Tensor  # type: ignore[misc]
    chosen_text: Optional[str] = None  # Text of the chosen response
    rejected_text: Optional[str] = None  # Text of the rejected response
    prompt_text: Optional[str] = None  # Text of the prompt

    @classmethod
    def gather(cls, experiences: List[DPOExperience], pad_token_id: int = 0) -> Experiences:
        """Gather a batch of DPO experiences from a list of experiences."""
        single_turn_experiences = []
        for exp in experiences:
            single_turn_experiences.append(
                SingleTurnExperience(
                    eid=EID(
                        batch=exp.eid.batch,
                        task=exp.eid.task,
                        step=exp.eid.step,
                    ),
                    token_ids=torch.cat([exp.token_ids, exp.chosen]),
                    reward=exp.reward,
                    info=exp.info,
                    metrics=exp.metrics,
                    prompt_length=len(exp.token_ids),
                    prompt_text=exp.prompt_text,
                    response_text=exp.chosen_text,
                )
            )
            single_turn_experiences.append(
                SingleTurnExperience(
                    eid=EID(
                        batch=exp.eid.batch,
                        task=exp.eid.task,
                        step=exp.eid.step,
                    ),
                    token_ids=torch.cat([exp.token_ids, exp.rejected]),
                    reward=exp.reward,
                    info=exp.info,
                    metrics=exp.metrics,
                    prompt_length=len(exp.token_ids),
                    prompt_text=exp.prompt_text,
                    response_text=exp.rejected_text,
                )
            )
        return SingleTurnExperience.gather(single_turn_experiences, pad_token_id=pad_token_id)


@dataclass(frozen=True)
class Experiences:
    """A container for a batch of experiences, for high performance communication usage.

    Example:

        >>>             |<- prompt_length ->|               |
        >>> token_ids: ('P' represents prompt, 'O' represents output)
        >>> exp1:       |........PPPPPPPPPPP|OOOOOOOOOO.....|
        >>> exp2:       |......PPPPPPPPPPPPP|OOOOOOO........|
        >>>
        >>> attention_masks: ('.' represents False and '1' represents True)
        >>> exp1:       |........11111111111|1111111111.....|
        >>> exp2:       |......1111111111111|1111111........|
    """

    eids: List[EID]  # Experience IDs of each experience in the batch
    token_ids: Tensor
    rewards: Tensor
    attention_masks: Tensor
    action_masks: Optional[Tensor]
    prompt_length: int
    logprobs: Optional[Tensor]

    @property
    def batch_size(self) -> int:
        """Get the batch size."""
        return self.token_ids.size(0)

    @classmethod
    def gather_experiences(
        cls, experiences: list[Experience], pad_token_id: int = 0
    ) -> Experiences:
        """Gather a batch of experiences from a list of experiences.

        This method will automatically pad the `tokens` and `logprobs` of input experiences to the same length.
        """
        if len(experiences) == 0:
            return empty_experiences()
        return experiences[0].__class__.gather(experiences, pad_token_id=pad_token_id)


def empty_experiences() -> Experiences:
    return Experiences(
        token_ids=torch.empty(0, dtype=torch.int32),
        rewards=torch.empty(0, dtype=torch.float32),
        attention_masks=torch.empty(0, dtype=torch.bool),
        action_masks=torch.empty(0, dtype=torch.bool),
        logprobs=torch.empty(0, dtype=torch.float32),
        prompt_length=torch.empty(0, dtype=torch.int32),
        eids=[],
    )


def gather_token_ids(
    experiences, max_prompt_length: int, max_response_length: int, pad_token_id: int
) -> Tensor:
    token_ids_dtype = experiences[0].token_ids.dtype
    return torch.stack(
        [
            torch.cat(
                [
                    torch.full(
                        (max_prompt_length - exp.prompt_length,),
                        pad_token_id,
                        dtype=token_ids_dtype,
                    ),
                    exp.tokens,
                    torch.full(
                        (max_response_length + exp.prompt_length - len(exp.tokens),),
                        pad_token_id,
                        dtype=token_ids_dtype,
                    ),
                ]
            )
            for exp in experiences
        ]
    )


def gather_action_masks(experiences, max_prompt_length: int, max_response_length: int) -> Tensor:
    return torch.stack(
        [
            torch.cat(
                [
                    torch.full(
                        (max_prompt_length - exp.prompt_length,),
                        0,
                        dtype=torch.bool,
                    ),
                    exp.action_mask,
                    torch.full(
                        (max_response_length + exp.prompt_length - len(exp.tokens),),
                        0,
                        dtype=torch.bool,
                    ),
                ]
            )
            for exp in experiences
        ]
    )


def gather_attention_masks(experiences, max_prompt_length: int, max_response_length: int) -> Tensor:
    attention_masks = torch.zeros(
        (len(experiences), max_prompt_length + max_response_length), dtype=torch.bool
    )

    for i, exp in enumerate(experiences):
        start = max_prompt_length - exp.prompt_length
        end = start + len(exp.tokens)
        attention_masks[i, start:end] = 1

    return attention_masks


def gather_logprobs(experiences, max_prompt_length: int, max_response_length: int) -> Tensor:
    logprob_dtype = experiences[0].logprobs.dtype  # type: ignore [union-attr]
    return torch.stack(
        [
            torch.cat(
                [
                    torch.full(
                        (max_prompt_length - exp.prompt_length,),
                        0.0,
                        dtype=logprob_dtype,
                    ),
                    exp.logprobs,
                    torch.full(
                        (max_response_length + exp.prompt_length - len(exp.tokens),),
                        0.0,
                        dtype=logprob_dtype,
                    ),
                ]
            )
            for exp in experiences
        ]
    )
