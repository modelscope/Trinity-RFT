import inspect
from abc import ABC, ABCMeta, abstractmethod
from typing import Dict, Tuple

import torch

from trinity.algorithm.key_mapper import ALL_MAPPERS
from trinity.utils.registry import Registry

POLICY_LOSS_FN = Registry("policy_loss_fn")


class PolicyLossFnMeta(ABCMeta):
    """Meta class for policy loss function."""

    ignore_keys = {"self", "kwargs", "logprob"}

    def __new__(cls, name, bases, dct):
        signature = inspect.signature(dct["__call__"])
        param_names = [
            key for key in signature.parameters.keys() if key not in PolicyLossFnMeta.ignore_keys
        ]
        dct["_select_keys"] = param_names

        def select_keys(self):
            keys = [self.mapper.from_trinity(key) for key in self._select_keys]
            return keys

        def decorator(func):
            def wrapper(self, *args, **kwargs):
                new_kwargs = {}
                for key, value in kwargs.items():
                    key = self.mapper.to_trinity(key)
                    if key in self._select_keys:  # remove unused keys
                        new_kwargs[key] = value
                kwargs = new_kwargs
                return func(self, *args, **new_kwargs)

            return wrapper

        dct["select_keys"] = property(select_keys)
        dct["__call__"] = decorator(dct["__call__"])
        return super().__new__(cls, name, bases, dct)


class PolicyLossFn(ABC, metaclass=PolicyLossFnMeta):
    """
    Policy Loss Function
    """

    def __init__(self, backend: str = "verl"):
        self.backend = backend
        self.mapper = ALL_MAPPERS[self.backend]

    @abstractmethod
    def __call__(
        self,
        logprob: torch.Tensor,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict]:
        """
        Args:
            logprob (`torch.Tensor`): The log probability generated by the policy model.
            old_logprob (`torch.Tensor`): The log probability generated by the reference model.
            action_mask (`torch.Tensor`): The action mask.
            advantages (`torch.Tensor`): The advantages.
            kwargs (`Dict`): The step-level parameters for calculating the policy loss.

        Returns:
            `torch.Tensor`: Policy loss
            `Dict`: The metrics for logging.
        """

    @classmethod
    @abstractmethod
    def default_args(cls) -> Dict:
        """
        Returns:
            `Dict`: The default init arguments for the policy loss function.
        """
